# Deeplearning_io
This repository includes my solutions for Andrew Ng's [Deeplearning Specialization](https://www.coursera.org/specializations/deep-learning) programming exercises.
You can see the projects by clicking the hyperlinks below.

**Course 1: Neural Networks and Deep Learning**
- [Python Basics with Numpy](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Python%2BBasics%2BWith%2BNumpy%2Bv3.ipynb) :
  - Implement Basic numpy functions, Broadcasting rule, Softmax, Vectorization, L1 and L2 loss.
- [Logistic Regression with a Neural Network mindset](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Logistic%2BRegression%2Bwith%2Ba%2BNeural%2BNetwork%2Bmindset%2Bv5.ipynb) :
  - Implement Logistic regression classifier to recognize cats. 
- [Planar data classification with one hidden layer](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Planar%2Bdata%2Bclassification%2Bwith%2Bone%2Bhidden%2Blayer%2Bv5.ipynb) : 
  - A 2-class classification neural network with a single hidden layer
  - Implement forward and backward propagation, tanh, and cross entropy loss gradient descent using pure numpy.
- [Building your Deep Neural Network: Step by Step](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Building%2Byour%2BDeep%2BNeural%2BNetwork%2B-%2BStep%2Bby%2BStep%2Bv8.ipynb)
  - Build L-layer neural network (L >= 1)
  - Design an easy-to-use neural network class. Use non-linear units like ReLU to improve model.
- [Deep Neural Network - Application](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Deep%2BNeural%2BNetwork%2B-%2BApplication%2Bv8.ipynb)
  - Use the L-layer neural network to classifies cat vs. non-cat images.
  
**Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization**
- [Initialization](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Initialization.ipynb)
  - Investigate Random initialization and He initialization performance on Deep Neural network.
- [Regularization](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Regularization%2B-%2Bv2.ipynb)
  - Implement L2 regularization, Dropout, and their forward and backward propagation algorithms. 
- [Gradient Checking](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Gradient%2BChecking%2Bv1.ipynb)
  - Implement gradient checking from scratch and use it to check backpropagation implementation.
- [Optimization](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Optimization%2Bmethods.ipynb)
  - Implement Stochastic Gradient Descent and Mini-Batch Gradient Descent.
  - Achieve Momentum and Adam optimiazation algorithms using exponentially weighted average. Compare their performance in different Gradient Descents. 
- [Tensorflow](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Tensorflow%2BTutorial.ipynb)
  - Learn how to implement Deep Neural Network using Tensorflow.

**Course 3: Structuring Machine Learning Projects**
- Does not have programming assignment.

**Course 4: Convolutional Neural Networks**
- [Convolutional Model: step by step](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Convolution%2Bmodel%2B-%2BStep%2Bby%2BStep%2B-%2Bv2.ipynb)
  - implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and backward propagation.
- [Convolutional Model: application](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Convolution%2Bmodel%2B-%2BApplication%2B-%2Bv1.ipynb)
  - Implement a fully functioning ConvNet using TensorFlow.
- [Keras Tutorial - The Happy House](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Keras%2B-%2BTutorial%2B-%2BHappy%2BHouse%2Bv2.ipynb)
  - Learn Keras by implementing ConvNet to detect Happy or not Happy face expression.
- [Residual Networks](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Residual%2BNetworks%2B-%2Bv2.ipynb)
  - Implement basic building blocks of ResNets to achieve the state-of-the-art neural network (50 layers) for image classification. 
- [Car detection with YOLOv2](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Autonomous%2Bdriving%2Bapplication%2B-%2BCar%2Bdetection%2B-%2Bv3.ipynb)
  - Implement YOLO object detection model on a car detection dataset and label cars by bounding boxes.
- [Face Recognition for the Happy House](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Face%2BRecognition%2Bfor%2Bthe%2BHappy%2BHouse%2B-%2Bv3.ipynb)
  - Implement the triplet loss function. And use a pretrained model to map face images into 128-dimensional encodings to perform face verification and face recognition.
- [Art generation with Neural Style Transfer](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Art%2BGeneration%2Bwith%2BNeural%2BStyle%2BTransfer%2B-%2Bv2.ipynb)
  - Implement the neural style transfer algorithm to generate novel artistic images.
  - Use the Transfer Learning idiom on VGG-19 to achieve this algorithm.

**Course 5: Sequence Models**
- [Building a recurrent neural network - step by step](https://github.com/XuShaoming/Deeplearning_io/blob/master/Sequence_Models/Building%2Ba%2BRecurrent%2BNeural%2BNetwork%2B-%2BStep%2Bby%2BStep%2B-%2Bv3.ipynb)
  - Implement both RNN and LSTM from scratch in numpy, including forward propagation and backward propagation.
- [Dinosaur Island - Character-Level Language Modeling](https://github.com/XuShaoming/Deeplearning_io/blob/master/Sequence_Models/Dinosaurus%2BIsland%2B--%2BCharacter%2Blevel%2Blanguage%2Bmodel%2Bfinal%2B-%2Bv3.ipynb)
  - Implement data preprocessing gradients clipping, output sampling to enable a trained RNN to generate Dinosaur Name.
- [Jazz improvisation with LSTM](https://github.com/XuShaoming/Deeplearning_io/blob/master/Sequence_Models/Improvise%2Ba%2BJazz%2BSolo%2Bwith%2Ban%2BLSTM%2BNetwork%2B-%2Bv3.py)
  - Build an LSTM model to generate music using Keras.
- [Operations on word vectors - Debiasing](https://github.com/XuShaoming/Deeplearning_io/blob/master/Sequence_Models/Operations%2Bon%2Bword%2Bvectors%2B-%2Bv2.ipynb)
  - Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __ by cosine similarity.
  - Modify word embeddings to reduce their gender bias.
- [Emojify](https://github.com/XuShaoming/Deeplearning_io/blob/master/Sequence_Models/Emojify%2B-%2Bv2.ipynb)
  - Implement word vector representations and a deep LSTM network to build an Emojifier using Keras. 
- [Neural Machine Translation with Attention](https://github.com/XuShaoming/Deeplearning_io/blob/master/Sequence_Models/Neural%2Bmachine%2Btranslation%2Bwith%2Battention%2B-%2Bv4.ipynb)
  - Build a Attention model, the state of the art Neural Machine Translation (NMT) model, to translate human readable dates ("25th of June, 2009") into machine readable dates ("2009-06-25").
- [Trigger word detection](https://uxmouwhrhztcdfzyddytkd.coursera-apps.org/notebooks/Week%203/Trigger%20word%20detection/Trigger%20word%20detection%20-%20v1.ipynb#)
  - Implement algorithms to synthesize and process audio recordings to create train/dev datasets.
  - Implement a model using 1D conv layer and deep GRU RNN to detect trigger word in audio recordings.


 

  
  
 

  






