# Deeplearning_io
This repository includes my solutions for Andrew Ng's [Deeplearning Specialization](https://www.coursera.org/specializations/deep-learning) programming exercises.
You can see the projects by clicking the hyperlinks below.

**Course 1: Neural Networks and Deep Learning**
- [Python Basics with Numpy](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Python%2BBasics%2BWith%2BNumpy%2Bv3.ipynb) :
  - Implement Basic numpy functions, Broadcasting rule, Softmax, Vectorization, L1 and L2 loss.
- [Logistic Regression with a Neural Network mindset](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Logistic%2BRegression%2Bwith%2Ba%2BNeural%2BNetwork%2Bmindset%2Bv5.ipynb) :
  - Implement Logistic regression classifier to recognize cats. 
- [Planar data classification with one hidden layer](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Planar%2Bdata%2Bclassification%2Bwith%2Bone%2Bhidden%2Blayer%2Bv5.ipynb) : 
  - A 2-class classification neural network with a single hidden layer
  - Implement forward and backward propagation, tanh, and cross entropy loss gradient descent using pure numpy.
- [Building your Deep Neural Network: Step by Step](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Building%2Byour%2BDeep%2BNeural%2BNetwork%2B-%2BStep%2Bby%2BStep%2Bv8.ipynb)
  - Build L-layer neural network (L >= 1)
  - Design an easy-to-use neural network class. Use non-linear units like ReLU to improve model.
- [Deep Neural Network - Application](https://github.com/XuShaoming/Deeplearning_io/blob/master/Neural_Networks_and_Deep_Learning/Deep%2BNeural%2BNetwork%2B-%2BApplication%2Bv8.ipynb)
  - Use the L-layer neural network to classifies cat vs. non-cat images.
  
**Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization**
- [Initialization](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Initialization.ipynb)
  - Investigate Random initialization and He initialization performance on Deep Neural network.
- [Regularization](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Regularization%2B-%2Bv2.ipynb)
  - Implement L2 regularization, Dropout, and their forward and backward propagation algorithms. 
- [Gradient Checking](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Gradient%2BChecking%2Bv1.ipynb)
  - Implement gradient checking from scratch and use it to check backpropagation implementation.
- [Optimization](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Optimization%2Bmethods.ipynb)
  - Implement Stochastic Gradient Descent and Mini-Batch Gradient Descent.
  - Achieve Momentum and Adam optimiazation algorithms using exponentially weighted average. Compare their performance in different Gradient Descents. 
- [Tensorflow](https://github.com/XuShaoming/Deeplearning_io/blob/master/Hyperparameter_tuning_Regularization_and_Optimization/Tensorflow%2BTutorial.ipynb)
  - Learn how to implement Deep Neural Network using Tensorflow.

**Course 3: Structuring Machine Learning Projects**
- Does not have programming assignment.

**Course 4: Convolutional Neural Networks**
- [Convolutional Model: step by step](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Convolution%2Bmodel%2B-%2BStep%2Bby%2BStep%2B-%2Bv2.ipynb)
  - implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and backward propagation.
- [Convolutional Model: application](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Convolution%2Bmodel%2B-%2BApplication%2B-%2Bv1.ipynb)
  - Implement a fully functioning ConvNet using TensorFlow.
- [Keras Tutorial - The Happy House](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Keras%2B-%2BTutorial%2B-%2BHappy%2BHouse%2Bv2.ipynb)
  - Learn Keras by implementing ConvNet to detect Happy or not Happy face expression.
- [Residual Networks](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Residual%2BNetworks%2B-%2Bv2.ipynb)
  - Implement basic building blocks of ResNets to achieve the state-of-the-art neural network (50 layers) for image classification. 
- [Car detection with YOLOv2](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Autonomous%2Bdriving%2Bapplication%2B-%2BCar%2Bdetection%2B-%2Bv3.ipynb)
  - Implement YOLO object detection model on a car detection dataset and label cars by bounding boxes.
- [Face Recognition for the Happy House](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Face%2BRecognition%2Bfor%2Bthe%2BHappy%2BHouse%2B-%2Bv3.ipynb)
  - Implement the triplet loss function. And use a pretrained model to map face images into 128-dimensional encodings to perform face verification and face recognition.
- [Art generation with Neural Style Transfer](https://github.com/XuShaoming/Deeplearning_io/blob/master/Convolutional_Neural_Networks/Art%2BGeneration%2Bwith%2BNeural%2BStyle%2BTransfer%2B-%2Bv2.ipynb)
  - Implement the neural style transfer algorithm to generate novel artistic images.
  - Use the Transfer Learning idiom on VGG-19 to achieve this algorithm.
- [Building a recurrent neural network - step by step](https://github.com/XuShaoming/Deeplearning_io/blob/master/Sequence_Models/Building%2Ba%2BRecurrent%2BNeural%2BNetwork%2B-%2BStep%2Bby%2BStep%2B-%2Bv3.ipynb)
  - Implement Recurrent Neural Network in numpy, including both forward propagation and backward propagation.
 

  
  
 

  






